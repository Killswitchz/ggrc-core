# Copyright (C) 2017 Google Inc.
# Licensed under http://www.apache.org/licenses/LICENSE-2.0 <see LICENSE file>

"""Automapper generator."""

from datetime import datetime
from logging import getLogger
import collections

import sqlalchemy as sa
from sqlalchemy.sql.expression import tuple_

from ggrc import db
from ggrc.automapper import rules
from ggrc import login
from ggrc.models.audit import Audit
from ggrc.models.automapping import Automapping
from ggrc.models.issue import Issue
from ggrc.models.relationship import Relationship
from ggrc.models import exceptions
from ggrc.rbac.permissions import is_allowed_update
from ggrc.services.common import get_cache
from ggrc.utils import benchmark


# pylint: disable=invalid-name
logger = getLogger(__name__)


class Stub(collections.namedtuple("Stub", ["type", "id"])):
  """Minimal object representation."""

  @classmethod
  def from_source(cls, relationship):
    return Stub(relationship.source_type, relationship.source_id)

  @classmethod
  def from_destination(cls, relationship):
    return Stub(relationship.destination_type, relationship.destination_id)


class AutomapperGenerator(object):
  """Generator for automappings.

  Consumes automapping rules and newly created Relationships, creates
  autogenerated Relationships registering them in Automappings table.

  Note: we can rely on the order of src/dst pairs of queued and
  inserted mappings since we only queue ordered pairs (see `order`).
  """

  COUNT_LIMIT = 10000

  def __init__(self):
    self.processed = set()
    self.queue = set()
    self.cache = collections.defaultdict(set)
    self.auto_mappings = set()

  def related(self, obj):
    if obj in self.cache:
      return self.cache[obj]

    # Pre-fetch neighborhood for enqueued objects since we're gonna need these
    # results in a few steps. This drastically reduces number of queries.
    stubs = {s for rel in self.queue for s in rel}
    stubs.add(obj)
    self._populate_cache(stubs)

    return self.cache[obj]

  def _populate_cache(self, stubs):
    """Fetch all mappings for objects in stubs, cache them in self.cache."""
    # Union is here to convince mysql to use two separate indices and
    # merge te results. Just using `or` results in a full-table scan
    # Manual column list avoids loading the full object which would also try to
    # load related objects
    cols = db.session.query(
        Relationship.source_type, Relationship.source_id,
        Relationship.destination_type, Relationship.destination_id)
    relationships = cols.filter(
        tuple_(Relationship.source_type, Relationship.source_id).in_(
            [(s.type, s.id) for s in stubs]
        )
    ).union_all(
        cols.filter(
            tuple_(Relationship.destination_type,
                   Relationship.destination_id).in_(
                       [(s.type, s.id) for s in stubs]))
    ).all()
    for (src_type, src_id, dst_type, dst_id) in relationships:
      src = Stub(src_type, src_id)
      dst = Stub(dst_type, dst_id)
      # only store a neighbor if we queried for it since this way we know
      # we'll be storing complete neighborhood by the end of the loop
      if src in stubs:
        self.cache[src].add(dst)
      if dst in stubs:
        self.cache[dst].add(src)

  @staticmethod
  def order(src, dst):
    return (src, dst) if src < dst else (dst, src)

  def generate_automappings(self, relationship):
    self.auto_mappings = set()
    with benchmark("Automapping generate_automappings"):
      # initial relationship is special since it is already created and
      # processing it would abort the loop so we manually enqueue the
      # neighborhood
      src = Stub.from_source(relationship)
      dst = Stub.from_destination(relationship)
      self._step(src, dst)
      self._step(dst, src)
      while self.queue:
        if len(self.auto_mappings) > self.COUNT_LIMIT:
          break
        src, dst = entry = self.queue.pop()

        if not (self._can_map_to(src, relationship) and
                self._can_map_to(dst, relationship)):
          continue

        created = self._ensure_relationship(src, dst)
        self.processed.add(entry)
        if not created:
          # If the edge already exists it means that auto mappings for it have
          # already been processed and it is safe to cut here.
          continue
        self._step(src, dst)
        self._step(dst, src)

      if len(self.auto_mappings) <= self.COUNT_LIMIT:
        self._flush(relationship)
      else:
        relationship._json_extras = {
            'automapping_limit_exceeded': True
        }

  @staticmethod
  def _can_map_to(obj, parent_relationship):
    return is_allowed_update(obj.type, obj.id, parent_relationship.context)

  def _flush(self, parent_relationship):
    """Manually INSERT generated automappings."""
    if not self.auto_mappings:
      return
    with benchmark("Automapping flush"):
      current_user_id = login.get_current_user_id()
      automapping_result = db.session.execute(
          Automapping.__table__.insert().values(
              relationship_id=parent_relationship.id,
              source_id=parent_relationship.source_id,
              source_type=parent_relationship.source_type,
              destination_id=parent_relationship.destination_id,
              destination_type=parent_relationship.destination_type,
          )
      )
      automapping_id = automapping_result.inserted_primary_key
      now = datetime.now()
      # We are doing an INSERT IGNORE INTO here to mitigate a race condition
      # that happens when multiple simultaneous requests create the same
      # automapping. If a relationship object fails our unique constraint
      # it means that the mapping was already created by another request
      # and we can safely ignore it.
      inserter = Relationship.__table__.insert().prefix_with("IGNORE")
      original = self.order(Stub.from_source(parent_relationship),
                            Stub.from_destination(parent_relationship))
      db.session.execute(inserter.values([{
          "id": None,
          "modified_by_id": current_user_id,
          "created_at": now,
          "updated_at": now,
          "source_id": src.id,
          "source_type": src.type,
          "destination_id": dst.id,
          "destination_type": dst.type,
          "context_id": None,
          "status": None,
          "parent_id": parent_relationship.id,
          "automapping_id": automapping_id}
          for src, dst in self.auto_mappings
          if (src, dst) != original]))  # (src, dst) is sorted

      self._set_audit_id_for_issues(automapping_id)

      cache = get_cache(create=True)
      if cache:
        # Add inserted relationships into new objects collection of the cache,
        # so that they will be logged within event and appropriate revisions
        # will be created.
        cache.new.update(
            (relationship, relationship.log_json())
            for relationship in Relationship.query.filter_by(
                automapping_id=automapping_id,
            )
        )

  @staticmethod
  def _set_audit_id_for_issues(automapping_id):
    """Set audit_id and context_id in automapped Issues."""
    iss, rel, aud = Issue.__table__, Relationship.__table__, Audit.__table__
    db.session.execute(
        sa.sql.expression.update(iss)
        .values({
            iss.c.audit_id: aud.c.id,
            iss.c.context_id: aud.c.context_id,
        })
        .where(
            sa.and_(
                rel.c.automapping_id == automapping_id,
                rel.c.source_type == Audit.__name__,
                rel.c.source_id == aud.c.id,
                rel.c.destination_type == Issue.__name__,
                rel.c.destination_id == iss.c.id,
            ),
        )
    )

  def _step(self, src, dst):
    mappings = rules.rules[src.type, dst.type]
    if mappings:
      dst_related = (o for o in self.related(dst)
                     if o.type in mappings and o != src)
      for r in dst_related:
        entry = self.order(r, src)
        if entry not in self.processed:
          self.queue.add(entry)

  def _ensure_relationship(self, src, dst):
    """Create the relationship if not exists already.

    Returns:
      True if a relationship was created;
      False otherwise.
    """
    # even though self.cache is defaultdict, self.cache[key] adds key
    # to self.cache if it is not present, and we should avoid it
    if dst in self.cache.get(src, set()):
      return False
    if src in self.cache.get(dst, set()):
      return False

    self._check_single_audit_restriction(src, dst)

    self.auto_mappings.add((src, dst))

    if src in self.cache:
      self.cache[src].add(dst)
    if dst in self.cache:
      self.cache[dst].add(src)

    return True

  def _check_single_audit_restriction(self, src, dst):
    """Fail if dst (Issue) is already mapped to an Audit."""
    # src, dst are ordered since they come from self.queue
    if (src.type, dst.type) == ("Audit", "Issue"):
      if "Audit" in (r.type for r in self.related(dst)):
        raise exceptions.ValidationError(
            "This request will result in automapping that will map "
            "Issue#{issue.id} to multiple Audits."
            .format(issue=dst)
        )


def register_automapping_listeners():
  """Register event listeners for auto mapper."""
  # pylint: disable=unused-variable,unused-argument

  def automap(session, _):
    automapper = AutomapperGenerator()
    for obj in session.new:
      if isinstance(obj, Relationship):
        automapper.generate_automappings(obj)

  sa.event.listen(sa.orm.session.Session, "after_flush", automap)
